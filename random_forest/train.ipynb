{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T06:33:05.989031Z",
     "start_time": "2024-08-24T06:33:05.973681Z"
    }
   },
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ydf\n",
    "import wandb"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "d899394a41c89ff0",
   "metadata": {},
   "source": [
    "Train a random forest model\n",
    "\n",
    "Current best model num_trees=500 & l2_regularization=0.00"
   ]
  },
  {
   "cell_type": "code",
   "id": "4e60231047d2536c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T06:19:18.513534Z",
     "start_time": "2024-08-24T06:18:54.240092Z"
    }
   },
   "source": [
    "FLAGS = dict(num_trees=50, \n",
    "             loss_fn=\"MULTINOMIAL_LOG_LIKELIHOOD\", \n",
    "             l2_penalty=0.001)\n",
    "\n",
    "wandb.init(\n",
    "    project = \"stonks\",\n",
    "    config = {\n",
    "        \"num_trees\": FLAGS['num_trees'],\n",
    "        \"l2_penalty\": FLAGS['l2_penalty'],\n",
    "        \"architecture\": \"GradientBoostedTrees\",\n",
    "    }\n",
    ")\n",
    "\n",
    "models = []\n",
    "# Loop through each set of training and validation data\n",
    "for i in range(11):  # Assuming i ranges from 0 to 12\n",
    "    # Load training and validation data\n",
    "    train_file = f\"covset0/unnorm/train_{i}.csv\"\n",
    "    valid_file = f\"covset0/unnorm/valid_{i}.csv\"\n",
    "\n",
    "    train_df = pd.read_csv(train_file)\n",
    "    valid_df = pd.read_csv(valid_file)\n",
    "\n",
    "    # Separate features and labels\n",
    "    X_train = train_df.drop(columns=['DELTA_20_QUINTILES'])  # Replace with actual label column\n",
    "    y_train = train_df['DELTA_20_QUINTILES']\n",
    "\n",
    "    X_valid = valid_df.drop(columns=['DELTA_20_QUINTILES'])  # Replace with actual label column\n",
    "    y_valid = valid_df['DELTA_20_QUINTILES']\n",
    "\n",
    "    # Combine X and y for YDF\n",
    "    train_data = pd.concat([X_train, y_train], axis=1)\n",
    "    valid_data = pd.concat([X_valid, y_valid], axis=1)\n",
    "\n",
    "    # Specify the label column for YDF\n",
    "    label = \"DELTA_20_QUINTILES\"\n",
    "\n",
    "    # Train the Gradient Boosted Trees model\n",
    "    model = ydf.GradientBoostedTreesLearner(task=ydf.Task.CLASSIFICATION, \n",
    "                                    label=label,\n",
    "                                    l2_regularization=FLAGS['l2_penalty'],\n",
    "                                    num_trees=FLAGS['num_trees'],\n",
    "                                    loss=FLAGS['loss_fn']).train(train_data, valid=valid_df)\n",
    "\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluation = model.evaluate(valid_data)\n",
    "    \n",
    "    models.append(model)\n",
    "\n",
    "    # Log the results with WandB\n",
    "    wandb.log({\n",
    "        'tv_set': i,\n",
    "        'Cross_Entropy': evaluation.loss,  # Log cross-entropy or other metrics\n",
    "    })\n",
    "\n",
    "    print(f\"Completed training and validation for fold {i}\")\n",
    "\n",
    "# Finalize WandB run\n",
    "wandb.finish()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Finishing last run (ID:7y4sknvn) before initializing another..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.016 MB uploaded (0.003 MB deduped)\\r'), FloatProgress(value=0.300292…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85314407a0b349408dbac94d5742f928"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "W&B sync reduced upload amount by 18.2%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gentle-snowball-31</strong> at: <a href='https://wandb.ai/osamkabinladna-prospera/stonks/runs/7y4sknvn' target=\"_blank\">https://wandb.ai/osamkabinladna-prospera/stonks/runs/7y4sknvn</a><br/> View project at: <a href='https://wandb.ai/osamkabinladna-prospera/stonks' target=\"_blank\">https://wandb.ai/osamkabinladna-prospera/stonks</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20240824_131601-7y4sknvn/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Successfully finished last run (ID:7y4sknvn). Initializing new run:<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011118268055555644, max=1.0…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29a9c3ad09514478aa525b6e6a92edb2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/Users/gregruyoga/gmoneycodes/tradingstrats/random_forest/wandb/run-20240824_131854-h7lf7n8c</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/osamkabinladna-prospera/stonks/runs/h7lf7n8c' target=\"_blank\">dashing-snowball-32</a></strong> to <a href='https://wandb.ai/osamkabinladna-prospera/stonks' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/osamkabinladna-prospera/stonks' target=\"_blank\">https://wandb.ai/osamkabinladna-prospera/stonks</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/osamkabinladna-prospera/stonks/runs/h7lf7n8c' target=\"_blank\">https://wandb.ai/osamkabinladna-prospera/stonks/runs/h7lf7n8c</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model on 252 training examples and 60 validation examples\n",
      "Model trained in 0:00:00.242555\n",
      "Completed training and validation for fold 0\n",
      "Train model on 252 training examples and 60 validation examples\n",
      "Model trained in 0:00:00.258244\n",
      "Completed training and validation for fold 1\n",
      "Train model on 252 training examples and 60 validation examples\n",
      "Model trained in 0:00:00.199882\n",
      "Completed training and validation for fold 2\n",
      "Train model on 252 training examples and 60 validation examples\n",
      "Model trained in 0:00:00.203694\n",
      "Completed training and validation for fold 3\n",
      "Train model on 252 training examples and 60 validation examples\n",
      "Model trained in 0:00:00.189347\n",
      "Completed training and validation for fold 4\n",
      "Train model on 252 training examples and 60 validation examples\n",
      "Model trained in 0:00:00.191913\n",
      "Completed training and validation for fold 5\n",
      "Train model on 252 training examples and 60 validation examples\n",
      "Model trained in 0:00:00.214292\n",
      "Completed training and validation for fold 6\n",
      "Train model on 252 training examples and 60 validation examples\n",
      "Model trained in 0:00:00.201037\n",
      "Completed training and validation for fold 7\n",
      "Train model on 252 training examples and 60 validation examples\n",
      "Model trained in 0:00:00.187035\n",
      "Completed training and validation for fold 8\n",
      "Train model on 252 training examples and 60 validation examples\n",
      "Model trained in 0:00:00.185480\n",
      "Completed training and validation for fold 9\n",
      "Train model on 252 training examples and 60 validation examples\n",
      "Model trained in 0:00:00.216774\n",
      "Completed training and validation for fold 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b8e51f8bb6a24a54b765593a875795c1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "W&B sync reduced upload amount by 16.7%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Cross_Entropy</td><td>▇█▁▁▂▁▆▂▅▂▂</td></tr><tr><td>tv_set</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Cross_Entropy</td><td>1.29303</td></tr><tr><td>tv_set</td><td>10</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dashing-snowball-32</strong> at: <a href='https://wandb.ai/osamkabinladna-prospera/stonks/runs/h7lf7n8c' target=\"_blank\">https://wandb.ai/osamkabinladna-prospera/stonks/runs/h7lf7n8c</a><br/> View project at: <a href='https://wandb.ai/osamkabinladna-prospera/stonks' target=\"_blank\">https://wandb.ai/osamkabinladna-prospera/stonks</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20240824_131854-h7lf7n8c/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "24c796ab9dcf58e3",
   "metadata": {},
   "source": [
    "model.predict_proba(valid)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "57ba6c834e301687",
   "metadata": {},
   "source": [
    "Train a Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "bdfd7cca453e32e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T06:20:11.467650Z",
     "start_time": "2024-08-24T06:20:10.808875Z"
    }
   },
   "source": [
    "# Loop through each set of training and validation data\n",
    "norm = False\n",
    "subdir = 'norm' if norm else 'unnorm'\n",
    "learners = []\n",
    "for i in range(11):  # Assuming i ranges from 0 to 12\n",
    "    # Load training and validation data\n",
    "    train_file = f\"covset0/{subdir}/train_{i}.csv\"\n",
    "    valid_file = f\"covset0/{subdir}/valid_{i}.csv\"\n",
    "\n",
    "    train_df = pd.read_csv(train_file)\n",
    "    valid_df = pd.read_csv(valid_file)\n",
    "\n",
    "    # Separate features and labels\n",
    "    X_train = train_df.drop(columns=['DELTA_20_QUINTILES'])  # Replace with actual label column\n",
    "    y_train = train_df['DELTA_20_QUINTILES']\n",
    "\n",
    "    X_valid = valid_df.drop(columns=['DELTA_20_QUINTILES'])  # Replace with actual label column\n",
    "    y_valid = valid_df['DELTA_20_QUINTILES']\n",
    "\n",
    "    # Combine X and y for YDF\n",
    "    train_data = pd.concat([X_train, y_train], axis=1)\n",
    "    valid_data = pd.concat([X_valid, y_valid], axis=1)\n",
    "\n",
    "    # Specify the label column for YDF\n",
    "    label = \"DELTA_20_QUINTILES\"\n",
    "\n",
    "    # Train the Gradient Boosted Trees model\n",
    "    learner = ydf.RandomForestLearner(task=ydf.Task.CLASSIFICATION,\n",
    "                                    label=label).train(train_df)\n",
    "\n",
    "    learners.append(learner)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model on 252 examples\n",
      "Model trained in 0:00:00.045383\n",
      "Train model on 252 examples\n",
      "Model trained in 0:00:00.039942\n",
      "Train model on 252 examples\n",
      "Model trained in 0:00:00.046810\n",
      "Train model on 252 examples\n",
      "Model trained in 0:00:00.046672\n",
      "Train model on 252 examples\n",
      "Model trained in 0:00:00.060897\n",
      "Train model on 252 examples\n",
      "Model trained in 0:00:00.057342\n",
      "Train model on 252 examples\n",
      "Model trained in 0:00:00.043092\n",
      "Train model on 252 examples\n",
      "Model trained in 0:00:00.038673\n",
      "Train model on 252 examples\n",
      "Model trained in 0:00:00.049157\n",
      "Train model on 252 examples\n",
      "Model trained in 0:00:00.035978\n",
      "Train model on 252 examples\n",
      "Model trained in 0:00:00.036918\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "193dd9651210210d",
   "metadata": {},
   "source": [
    "model = learners[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f3a05a1a65a0b953",
   "metadata": {},
   "source": [
    "dir(model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "638695c41f45ca58",
   "metadata": {},
   "source": [
    "model.variable_importances()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "965bc8a9821aea71",
   "metadata": {},
   "source": [
    "model.predict(valid_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ed4bc0f531180fc8",
   "metadata": {},
   "source": [
    "evaluation = learners[0].analyze(valid_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9cc971db90910ef9",
   "metadata": {},
   "source": [
    "evaluation"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5636d2d3e99c507",
   "metadata": {},
   "source": [
    "model.evaluate(valid_data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1b1f8b2b3011bb0f",
   "metadata": {},
   "source": [
    "Combine all in sample data into train_data \n",
    "and all out of sample data into valid_data "
   ]
  },
  {
   "cell_type": "code",
   "id": "b08de9201e9eeb7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T06:20:31.635315Z",
     "start_time": "2024-08-24T06:20:30.392742Z"
    }
   },
   "source": [
    "import wandb\n",
    "import ydf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# wandb.init(project='stonks')\n",
    "\n",
    "norm = False\n",
    "learners = []\n",
    "subdir = 'norm' if norm else 'unnorm'\n",
    "\n",
    "tdf = []\n",
    "vdf = []\n",
    "for i in range(11):\n",
    "    train_df = pd.read_csv(f'covset0/{subdir}/train_{i}.csv')\n",
    "    valid_df = pd.read_csv(f'covset0/{subdir}/valid_{i}.csv')\n",
    "    \n",
    "    tdf.append(train_df)\n",
    "    vdf.append(valid_df)\n",
    "\n",
    "train_data = pd.concat(tdf, axis=0)\n",
    "valid_data = pd.concat(vdf, axis=0)\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "55fdef7d927996f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T06:20:34.393888Z",
     "start_time": "2024-08-24T06:20:33.564497Z"
    }
   },
   "source": [
    "label = 'DELTA_20_QUINTILES'\n",
    "learner = ydf.RandomForestLearner(task=ydf.Task.CLASSIFICATION, label=label, num_trees=300).train(train_data)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model on 2772 examples\n",
      "Model trained in 0:00:00.794933\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T06:43:01.553182Z",
     "start_time": "2024-08-24T06:40:04.578519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "label = 'DELTA_20_QUINTILES'\n",
    "\n",
    "# learner = ydf.RandomForestLearner(task=ydf.Task.CLASSIFICATION, label=label, num_trees=10000,\n",
    "#                                   winner_take_all=False, growing_strategy='BEST_FIRST_GLOBAL').train(train_data)\n",
    "\n",
    "learner = (ydf.RandomForestLearner(task=ydf.Task.CLASSIFICATION,\n",
    "                                        label=label,\n",
    "                                        categorical_algorithm='CART',\n",
    "                                        max_depth = 25,\n",
    "                                        # growing_strategy='BEST_FIRST_GLOBAL',\n",
    "                                        num_trees=50000).train(train_data))\n",
    "\n",
    "valid_preds = learner.predict(valid_data)\n",
    "preds = pd.DataFrame(valid_preds, columns=learner.label_classes())\n",
    "preds['Probs'] = preds.max(axis=1)\n",
    "preds['Predicted'] = preds.idxmax(axis=1)\n",
    "\n",
    "# Ensure consistency in lengths and alignment\n",
    "true_classes = valid_data[\"DELTA_20_QUINTILES\"].reset_index(drop=True)\n",
    "predicted_classes = preds['Predicted'].astype(int).reset_index(drop=True)\n",
    "\n",
    "# Check if lengths match\n",
    "assert len(true_classes) == len(predicted_classes), \"Lengths of true and predicted classes do not match.\"\n",
    "\n",
    "# Create the filter mask for classes 1 and 5\n",
    "filter_mask = (true_classes.isin([1, 5])) | (predicted_classes.isin([1, 5]))\n",
    "\n",
    "# Apply the filter\n",
    "filtered_tclass = true_classes[filter_mask]\n",
    "filtered_pclass = predicted_classes[filter_mask]\n",
    "\n",
    "# Calculate accuracies\n",
    "total_accuracy = accuracy_score(true_classes, predicted_classes)\n",
    "onefive_accuracy = accuracy_score(filtered_tclass, filtered_pclass)\n",
    "\n",
    "print(\"Accuracy: \", total_accuracy)\n",
    "print(\"1/5 accuracy: \", onefive_accuracy)\n",
    "\n",
    "# Define the penalty matrix\n",
    "weight = 0.5\n",
    "penalty_matrix = np.array([\n",
    "    [0, weight, 1, 1, 1],   # True class is 1\n",
    "    [weight, 0, 1, 1, 1], # True class is 2\n",
    "    [1, 1, 0, 1, 1], # True class is 3\n",
    "    [1, 1, 1, 0, weight], # True class is 4\n",
    "    [1, 1, 1, weight, 0],   # True class is 5\n",
    "])\n",
    "\n",
    "# Convert the true and predicted classes to numpy arrays for easier indexing\n",
    "true_classes = valid_data[\"DELTA_20_QUINTILES\"].to_numpy()\n",
    "predicted_classes = preds['Predicted'].astype(int).to_numpy()\n",
    "\n",
    "# Initialize a list to store penalties for each prediction\n",
    "penalties = []\n",
    "\n",
    "# Loop through each prediction and calculate the penalty\n",
    "for true_class, pred_class in zip(true_classes, predicted_classes):\n",
    "    penalty = penalty_matrix[true_class - 1, pred_class - 1]\n",
    "    penalties.append(penalty)\n",
    "\n",
    "# Calculate total weighted accuracy\n",
    "weighted_accuracy = 1 - np.mean(penalties)\n",
    "\n",
    "# Print the weighted accuracy\n",
    "print(\"Coping Accuracy: \", weighted_accuracy)"
   ],
   "id": "adf48f9a2be003a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model on 2772 examples\n",
      "Model trained in 0:02:00.355458\n",
      "Accuracy:  0.543939393939394\n",
      "1/5 accuracy:  0.5365168539325843\n",
      "Coping Accuracy:  0.6409090909090909\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T06:29:52.721503Z",
     "start_time": "2024-08-24T06:29:52.713174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the penalty matrix\n",
    "5424\n",
    "weight = 0.75\n",
    "penalty_matrix = np.array([\n",
    "    [0, weight, 1, 1, 1],   # True class is 1\n",
    "    [weight, 0, 1, 1, 1], # True class is 2\n",
    "    [1, 1, 0, 1, 1], # True class is 3\n",
    "    [1, 1, 1, 0, weight], # True class is 4\n",
    "    [1, 1, 1, weight, 0],   # True class is 5\n",
    "])\n",
    "\n",
    "# Convert the true and predicted classes to numpy arrays for easier indexing\n",
    "true_classes = valid_data[\"DELTA_20_QUINTILES\"].to_numpy()\n",
    "predicted_classes = preds['Predicted'].astype(int).to_numpy()\n",
    "\n",
    "# Initialize a list to store penalties for each prediction\n",
    "penalties = []\n",
    "\n",
    "# Loop through each prediction and calculate the penalty\n",
    "for true_class, pred_class in zip(true_classes, predicted_classes):\n",
    "    penalty = penalty_matrix[true_class - 1, pred_class - 1]\n",
    "    penalties.append(penalty)\n",
    "\n",
    "# Calculate total weighted accuracy\n",
    "weighted_accuracy = 1 - np.mean(penalties)\n",
    "\n",
    "# Print the weighted accuracy\n",
    "print(\"Coping Accuracy: \", weighted_accuracy)"
   ],
   "id": "ff38c0a0785851cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coping Accuracy:  0.5852272727272727\n"
     ]
    }
   ],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
