{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import ydf\n",
    "import wandb"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d899394a41c89ff0",
   "metadata": {},
   "source": [
    "Train a random forest model\n",
    "\n",
    "Current best model num_trees=500 & l2_regularization=0.00"
   ]
  },
  {
   "cell_type": "code",
   "id": "4e60231047d2536c",
   "metadata": {},
   "source": [
    "FLAGS = dict(num_trees=50, \n",
    "             loss_fn=\"MULTINOMIAL_LOG_LIKELIHOOD\", \n",
    "             l2_penalty=0.001)\n",
    "\n",
    "wandb.init(\n",
    "    project = \"stonks\",\n",
    "    config = {\n",
    "        \"num_trees\": FLAGS['num_trees'],\n",
    "        \"l2_penalty\": FLAGS['l2_penalty'],\n",
    "        \"architecture\": \"GradientBoostedTrees\",\n",
    "    }\n",
    ")\n",
    "\n",
    "models = []\n",
    "# Loop through each set of training and validation data\n",
    "for i in range(11):  # Assuming i ranges from 0 to 12\n",
    "    # Load training and validation data\n",
    "    train_file = f\"covset0/train_{i}.csv\"\n",
    "    valid_file = f\"covset0/valid_{i}.csv\"\n",
    "\n",
    "    train_df = pd.read_csv(train_file)\n",
    "    valid_df = pd.read_csv(valid_file)\n",
    "\n",
    "    # Separate features and labels\n",
    "    X_train = train_df.drop(columns=['DELTA_20_QUINTILES'])  # Replace with actual label column\n",
    "    y_train = train_df['DELTA_20_QUINTILES']\n",
    "\n",
    "    X_valid = valid_df.drop(columns=['DELTA_20_QUINTILES'])  # Replace with actual label column\n",
    "    y_valid = valid_df['DELTA_20_QUINTILES']\n",
    "\n",
    "    # Combine X and y for YDF\n",
    "    train_data = pd.concat([X_train, y_train], axis=1)\n",
    "    valid_data = pd.concat([X_valid, y_valid], axis=1)\n",
    "\n",
    "    # Specify the label column for YDF\n",
    "    label = \"DELTA_20_QUINTILES\"\n",
    "\n",
    "    # Train the Gradient Boosted Trees model\n",
    "    model = ydf.GradientBoostedTreesLearner(task=ydf.Task.CLASSIFICATION, \n",
    "                                    label=label,\n",
    "                                    l2_regularization=FLAGS['l2_penalty'],\n",
    "                                    num_trees=FLAGS['num_trees'],\n",
    "                                    loss=FLAGS['loss_fn']).train(train_data, valid=valid_df)\n",
    "\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluation = model.evaluate(valid_data)\n",
    "    \n",
    "    models.append(model)\n",
    "\n",
    "    # Log the results with WandB\n",
    "    wandb.log({\n",
    "        'tv_set': i,\n",
    "        'Cross_Entropy': evaluation.loss,  # Log cross-entropy or other metrics\n",
    "    })\n",
    "\n",
    "    print(f\"Completed training and validation for fold {i}\")\n",
    "\n",
    "# Finalize WandB run\n",
    "wandb.finish()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "24c796ab9dcf58e3",
   "metadata": {},
   "source": [
    "model.predict_proba(valid)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "57ba6c834e301687",
   "metadata": {},
   "source": [
    "Train a Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "bdfd7cca453e32e1",
   "metadata": {},
   "source": [
    "# Loop through each set of training and validation data\n",
    "norm = False\n",
    "subdir = 'norm' if norm else 'unnorm'\n",
    "learners = []\n",
    "for i in range(11):  # Assuming i ranges from 0 to 12\n",
    "    # Load training and validation data\n",
    "    train_file = f\"covset0/{subdir}/train_{i}.csv\"\n",
    "    valid_file = f\"covset0/{subdir}/valid_{i}.csv\"\n",
    "\n",
    "    train_df = pd.read_csv(train_file)\n",
    "    valid_df = pd.read_csv(valid_file)\n",
    "\n",
    "    # Separate features and labels\n",
    "    X_train = train_df.drop(columns=['DELTA_20_QUINTILES'])  # Replace with actual label column\n",
    "    y_train = train_df['DELTA_20_QUINTILES']\n",
    "\n",
    "    X_valid = valid_df.drop(columns=['DELTA_20_QUINTILES'])  # Replace with actual label column\n",
    "    y_valid = valid_df['DELTA_20_QUINTILES']dd\n",
    "\n",
    "    # Combine X and y for YDF\n",
    "    train_data = pd.concat([X_train, y_train], axis=1)\n",
    "    valid_data = pd.concat([X_valid, y_valid], axis=1)\n",
    "\n",
    "    # Specify the label column for YDF\n",
    "    label = \"DELTA_20_QUINTILES\"\n",
    "\n",
    "    # Train the Gradient Boosted Trees model\n",
    "    learner = ydf.RandomForestLearner(task=ydf.Task.CLASSIFICATION,\n",
    "                                    label=label).train(train_df)\n",
    "\n",
    "    learners.append(learner)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "193dd9651210210d",
   "metadata": {},
   "source": [
    "model = learners[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f3a05a1a65a0b953",
   "metadata": {},
   "source": [
    "dir(model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "638695c41f45ca58",
   "metadata": {},
   "source": [
    "model.variable_importances()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "965bc8a9821aea71",
   "metadata": {},
   "source": [
    "model.predict(valid_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ed4bc0f531180fc8",
   "metadata": {},
   "source": [
    "evaluation = learners[0].analyze(valid_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9cc971db90910ef9",
   "metadata": {},
   "source": [
    "evaluation"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5636d2d3e99c507",
   "metadata": {},
   "source": [
    "model.evaluate(valid_data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1b1f8b2b3011bb0f",
   "metadata": {},
   "source": [
    "Combine all in sample data into train_data \n",
    "and all out of sample data into valid_data "
   ]
  },
  {
   "cell_type": "code",
   "id": "b08de9201e9eeb7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T07:50:19.087678Z",
     "start_time": "2024-08-22T07:50:17.015821Z"
    }
   },
   "source": [
    "import wandb\n",
    "import ydf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# wandb.init(project='stonks')\n",
    "\n",
    "norm = False\n",
    "learners = []\n",
    "subdir = 'norm' if norm else 'unnorm'\n",
    "\n",
    "tdf = []\n",
    "vdf = []\n",
    "for i in range(11):\n",
    "    train_df = pd.read_csv(f'covset0/{subdir}/train_{i}.csv')\n",
    "    valid_df = pd.read_csv(f'covset0/{subdir}/valid_{i}.csv')\n",
    "    \n",
    "    tdf.append(train_df)\n",
    "    vdf.append(valid_df)\n",
    "\n",
    "train_data = pd.concat(tdf, axis=0)\n",
    "valid_data = pd.concat(vdf, axis=0)\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "55fdef7d927996f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T07:50:22.490205Z",
     "start_time": "2024-08-22T07:50:21.210506Z"
    }
   },
   "source": [
    "label = 'DELTA_20_QUINTILES'\n",
    "learner = ydf.RandomForestLearner(task=ydf.Task.CLASSIFICATION, label=label, num_trees=300).train(train_data)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model on 2772 examples\n",
      "Model trained in 0:00:01.264144\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T08:16:54.569023Z",
     "start_time": "2024-08-22T08:16:53.173717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "label = 'DELTA_20_QUINTILES'\n",
    "\n",
    "# learner = ydf.RandomForestLearner(task=ydf.Task.CLASSIFICATION, label=label, num_trees=10000,\n",
    "#                                   winner_take_all=False, growing_strategy='BEST_FIRST_GLOBAL').train(train_data)\n",
    "\n",
    "learner = ydf.GradientBoostedTreesLearner(task=ydf.Task.CLASSIFICATION,\n",
    "                                        label=label,\n",
    "                                        l2_regularization=0.000,\n",
    "                                        growing_strategy='BEST_FIRST_GLOBAL',\n",
    "                                        num_trees=300,\n",
    "                                        loss='MULTINOMIAL_LOG_LIKELIHOOD').train(train_data, valid=valid_data)\n",
    "\n",
    "valid_preds = learner.predict(valid_data)\n",
    "preds = pd.DataFrame(valid_preds, columns=learner.label_classes())\n",
    "preds['Probs'] = preds.max(axis=1)\n",
    "preds['Predicted'] = preds.idxmax(axis=1)\n",
    "\n",
    "# Ensure consistency in lengths and alignment\n",
    "true_classes = valid_data[\"DELTA_20_QUINTILES\"].reset_index(drop=True)\n",
    "predicted_classes = preds['Predicted'].astype(int).reset_index(drop=True)\n",
    "\n",
    "# Check if lengths match\n",
    "assert len(true_classes) == len(predicted_classes), \"Lengths of true and predicted classes do not match.\"\n",
    "\n",
    "# Create the filter mask for classes 1 and 5\n",
    "filter_mask = (true_classes.isin([1, 5])) | (predicted_classes.isin([1, 5]))\n",
    "\n",
    "# Apply the filter\n",
    "filtered_tclass = true_classes[filter_mask]\n",
    "filtered_pclass = predicted_classes[filter_mask]\n",
    "\n",
    "# Calculate accuracies\n",
    "total_accuracy = accuracy_score(true_classes, predicted_classes)\n",
    "onefive_accuracy = accuracy_score(filtered_tclass, filtered_pclass)\n",
    "\n",
    "print(\"Accuracy: \", total_accuracy)\n",
    "print(\"1/5 accuracy: \", onefive_accuracy)"
   ],
   "id": "adf48f9a2be003a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model on 2772 training examples and 660 validation examples\n",
      "Model trained in 0:00:01.363003\n",
      "Accuracy:  0.5151515151515151\n",
      "1/5 accuracy:  0.5128939828080229\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T05:20:34.635923Z",
     "start_time": "2024-08-23T05:20:34.599151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the penalty matrix\n",
    "penalty_matrix = np.array([\n",
    "    [0, 0.5, 1, 1, 1],   # True class is 1\n",
    "    [0.5, 0, 0.5, 1, 1], # True class is 2\n",
    "    [1, 1, 0, 1, 1], # True class is 3\n",
    "    [1, 1, 0.5, 0, 0.5], # True class is 4\n",
    "    [1, 1, 1, 0.5, 0],   # True class is 5\n",
    "])\n",
    "\n",
    "# Convert the true and predicted classes to numpy arrays for easier indexing\n",
    "true_classes = valid_data[\"DELTA_20_QUINTILES\"].to_numpy()\n",
    "predicted_classes = preds['Predicted'].astype(int).to_numpy()\n",
    "\n",
    "# Initialize a list to store penalties for each prediction\n",
    "penalties = []\n",
    "\n",
    "# Loop through each prediction and calculate the penalty\n",
    "for true_class, pred_class in zip(true_classes, predicted_classes):\n",
    "    penalty = penalty_matrix[true_class - 1, pred_class - 1]\n",
    "    penalties.append(penalty)\n",
    "\n",
    "# Calculate total weighted accuracy\n",
    "weighted_accuracy = 1 - np.mean(penalties)\n",
    "\n",
    "# Print the weighted accuracy\n",
    "print(\"Coping Accuracy: \", weighted_accuracy)"
   ],
   "id": "ff38c0a0785851cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coping Accuracy:  0.6522727272727273\n"
     ]
    }
   ],
   "execution_count": 32
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
